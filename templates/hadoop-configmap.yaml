apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -eo pipefail
    set -u

    HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop}
    CONFIG_DIR="/tmp/hadoop-config"
    NN_WEB_PORT=9870
    DN_HTTP_PORT=9864
    DN_DATA_PORT=9866

    log() {
      level=$1
      message=$2
      echo "[$(date +'%Y-%m-%d %H:%M:%S')] [${level}] ${message}"
    }

    log "INFO" "启动Hadoop组件初始化，HADOOP_HOME=${HADOOP_HOME}"

    mkdir -p /opt/hadoop/logs
    chmod 777 /opt/hadoop/logs

    if [ -f "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh" ]; then
      . "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh"
    else
      log "ERROR" "未找到hadoop-env.sh"
      exit 1
    fi

    log "INFO" "复制配置文件"
    required_files=("core-site.xml" "hdfs-site.xml")
    for file in "${required_files[@]}"; do
      if [ ! -f "${CONFIG_DIR}/${file}" ]; then
        log "ERROR" "缺少配置文件 ${file}"
        exit 1
      fi
      cp -f "${CONFIG_DIR}/${file}" "${HADOOP_HOME}/etc/hadoop/${file}"
    done

    if [ -f "${CONFIG_DIR}/slaves" ]; then
      cp -f "${CONFIG_DIR}/slaves" "${HADOOP_HOME}/etc/hadoop/slaves"
      log "INFO" "已复制 slaves 文件"
    else
      log "WARN" "未找到 slaves 文件，跳过"
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      log "INFO" "启动DataNode"
      log "INFO" "DataNode配置：HTTP端口=${DN_HTTP_PORT}, 数据端口=${DN_DATA_PORT}"
      
      mkdir -p /data/hdfs/datanode
      log "INFO" "创建数据目录: /data/hdfs/datanode"

      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      log "INFO" "更新配置文件端口配置"

      log "INFO" "等待NameNode就绪..."
      nn_ready=false
      REPLICA_COUNT={{ .Values.hdfs.nameNode.replicas | int }}
      log "INFO" "检测 ${REPLICA_COUNT} 个NameNode"
      
      for i in $(seq 0 $((REPLICA_COUNT - 1))); do
        nn_service="{{ include "hadoop.fullname" . }}-hdfs-nn-${i}.{{ include "hadoop.fullname" . }}-hdfs-nn"
        tmp_url="http://${nn_service}:${NN_WEB_PORT}"
        log "INFO" "检测NameNode: ${nn_service}"
        
        for retry in {1..300}; do
          if curl -sf "${tmp_url}"; then
            log "INFO" "NameNode ${nn_service} 已就绪 (重试 ${retry}/300)"
            nn_ready=true
            break 2
          else
            log "INFO" "NameNode ${nn_service} 未就绪，继续等待 (重试 ${retry}/300)"
          fi
          sleep 10
        done
        if [ "$nn_ready" = true ]; then
          break
        fi
      done

      if [ "$nn_ready" = true ]; then
        log "INFO" "NameNode就绪，启动DataNode服务"
        log "INFO" "DataNode将连接到NameNode进行注册"
        "${HADOOP_HOME}/bin/hdfs" datanode
      else
        log "ERROR" "所有NameNode都未就绪，DataNode无法启动"
        log "ERROR" "请检查NameNode状态和网络连接"
        exit 1
      fi
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-jn" ]]; then
      log "INFO" "启动JournalNode"
      log "INFO" "JournalNode用于存储NameNode的编辑日志，支持HA"
      
      mkdir -p /data/hdfs/journalnode
      log "INFO" "创建JournalNode数据目录: /data/hdfs/journalnode"
      
      # 检查是否需要强制格式化
      FORCE_FORMAT_JN={{ .Values.formatting.forceFormatJournalNode | default false }}
      if [ "$FORCE_FORMAT_JN" = "true" ] || [ ! -f /data/hdfs/journalnode/min-hadoop/current/VERSION ]; then
        log "INFO" "格式化JournalNode存储目录"
        # 停止可能运行的 JournalNode 进程
        pkill -f journalnode || true
        sleep 2
        # 清理旧数据
        rm -rf /data/hdfs/journalnode/min-hadoop
        # 确保目录存在
        mkdir -p /data/hdfs/journalnode/min-hadoop
        # 格式化JournalNode
        "${HADOOP_HOME}/bin/hdfs" journalnode -format
        log "INFO" "JournalNode格式化完成"
      else
        log "INFO" "JournalNode已格式化，跳过格式化步骤"
      fi
      
      log "INFO" "启动JournalNode服务"
      log "INFO" "JournalNode将在端口8480提供HTTP服务，8485提供RPC服务"
      
      # 验证格式化是否成功
      if [ -f /data/hdfs/journalnode/min-hadoop/current/VERSION ]; then
        log "INFO" "JournalNode格式化验证成功，启动服务"
        "${HADOOP_HOME}/bin/hdfs" journalnode
      else
        log "ERROR" "JournalNode格式化失败，无法启动服务"
        exit 1
      fi
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      log "INFO" "启动NameNode"
      log "INFO" "NameNode是HDFS的主节点，负责元数据管理"
      
      mkdir -p /data/hdfs/namenode
      log "INFO" "创建NameNode数据目录: /data/hdfs/namenode"

      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      log "INFO" "更新DataNode端口配置"

      nn_index=$(echo "${HOSTNAME}" | grep -o '[0-9]\+$')
      export HDFS_NAMENODE_OPTS="${HDFS_NAMENODE_OPTS:-} -Ddfs.ha.namenode.id=nn${nn_index} -Ddfs.namenode.rpc-bind-host=0.0.0.0 -Ddfs.namenode.http-bind-host=0.0.0.0"
      log "INFO" "设置NameNode ID: nn${nn_index}"
      log "INFO" "设置NameNode绑定地址: 0.0.0.0"
      
      # 动态更新hdfs-site.xml中的namenode.id配置
      sed -i "s/nn\${nn_index}/nn${nn_index}/g" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      log "INFO" "已更新hdfs-site.xml中的namenode.id为: nn${nn_index}"

      # 等待所有JournalNode就绪
      log "INFO" "等待所有JournalNode就绪..."
      jn_ready=false
      JN_REPLICA_COUNT={{ .Values.hdfs.journalNode.replicas | int }}
      log "INFO" "需要等待所有 ${JN_REPLICA_COUNT} 个JournalNode就绪"
      
      for retry in {1..600}; do
        jn_ready_count=0
        for i in $(seq 0 $((JN_REPLICA_COUNT - 1))); do
          jn_service="{{ include "hadoop.fullname" . }}-hdfs-jn-${i}.{{ include "hadoop.fullname" . }}-hdfs-jn"
          if timeout 5 bash -c "</dev/tcp/${jn_service}/8485" 2>/dev/null; then
            log "INFO" "JournalNode ${jn_service} 已就绪"
            jn_ready_count=$((jn_ready_count + 1))
          fi
        done
        
        if [ "$jn_ready_count" -eq "$JN_REPLICA_COUNT" ]; then
          log "INFO" "所有 ${JN_REPLICA_COUNT} 个JournalNode都已就绪"
          jn_ready=true
          break
        else
          log "INFO" "当前有 ${jn_ready_count}/${JN_REPLICA_COUNT} 个JournalNode就绪，继续等待（重试 ${retry}/600）"
        fi
        sleep 10
      done

      if [ "$jn_ready" = true ]; then
        log "INFO" "JournalNode就绪，开始启动NameNode"
        
        # 检查是否需要强制格式化
        FORCE_FORMAT_NN={{ .Values.formatting.forceFormatNameNode | default false }}
        if [ "$FORCE_FORMAT_NN" = "true" ] || [ ! -f /data/hdfs/namenode/formated ]; then
          if [ "$nn_index" = "0" ]; then
            log "INFO" "格式化第一个NameNode (Active)"
            log "INFO" "这将创建新的HDFS文件系统"
            # 清理旧数据
            rm -rf /data/hdfs/namenode/current
            rm -rf /data/hdfs/namenode/formated
            "${HADOOP_HOME}/bin/hdfs" namenode -format -force -nonInteractive
            touch /data/hdfs/namenode/formated
            log "INFO" "NameNode格式化完成"
          else
            log "INFO" "启动第二个NameNode（Standby模式）"
            # 清理旧数据
            rm -rf /data/hdfs/namenode/current
            rm -rf /data/hdfs/namenode/formated
          fi
        else
          log "INFO" "NameNode已格式化，跳过格式化步骤"
        fi

        log "INFO" "启动NameNode服务"
        log "INFO" "NameNode将在端口9000提供RPC服务，9870提供HTTP服务"
        "${HADOOP_HOME}/bin/hdfs" namenode
      else
        log "ERROR" "JournalNode未就绪，NameNode无法启动"
        log "ERROR" "请检查JournalNode状态和网络连接"
        exit 1
      fi
    fi

    if [[ "${1:-}" == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>{{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}{{ if $i }},{{ end }}nn{{ $i }}{{- end }}</value>
      </property>
      {{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}
      <property>
        <name>dfs.namenode.rpc-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9870</value>
      </property>
      {{- end }}
      <property>
        <name>dfs.client.failover.proxy.provider.{{ .Values.hdfs.clusterName }}</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>{{ .Values.hdfs.replication | default 3 }}</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>{{ .Values.hdfs.webhdfs.enabled | default false }}</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/hdfs/datanode</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/hdfs/namenode</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/hdfs/journalnode</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.datanode.hostname</name>
        <value>${HOSTNAME}</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>{{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}{{ if $i }},{{ end }}nn{{ $i }}{{- end }}</value>
      </property>
      {{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}
      <property>
        <name>dfs.namenode.rpc-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9870</value>
      </property>
      {{- end }}
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{{- range $i, $e := until (.Values.hdfs.journalNode.replicas | int) }}{{ if $i }};{{ end }}{{ include "hadoop.fullname" $ }}-hdfs-jn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-jn:8485{{- end }}/{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
      </property>
      <property>
        <name>dfs.ha.namenode.id</name>
        <value>nn${nn_index}</value>
      </property>
      <property>
        <name>dfs.ha.initialization.timeout</name>
        <value>300</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.https-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
    </configuration>

  slaves: |
{{- range $i, $e := until (.Values.hdfs.dataNode.replicas | int) }}
    {{ include "hadoop.fullname" $ }}-hdfs-dn-{{ $i }}
{{- end }} 