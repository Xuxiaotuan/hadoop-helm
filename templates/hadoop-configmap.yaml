apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -eo pipefail
    set -u

    HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop}
    CONFIG_DIR="/tmp/hadoop-config"
    NN_WEB_PORT=9870
    DN_HTTP_PORT=9864
    DN_DATA_PORT=9866

    log() {
      level=$1
      message=$2
      echo "[$(date +'%Y-%m-%d %H:%M:%S')] [${level}] ${message}"
    }

    log "INFO" "启动Hadoop组件初始化，HADOOP_HOME=${HADOOP_HOME}"

    mkdir -p /opt/hadoop/logs
    chmod 777 /opt/hadoop/logs

    if [ -f "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh" ]; then
      . "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh"
    else
      log "ERROR" "未找到hadoop-env.sh"
      exit 1
    fi

    log "INFO" "复制配置文件"
    required_files=("core-site.xml" "hdfs-site.xml")
    for file in "${required_files[@]}"; do
      if [ ! -f "${CONFIG_DIR}/${file}" ]; then
        log "ERROR" "缺少配置文件 ${file}"
        exit 1
      fi
      cp -f "${CONFIG_DIR}/${file}" "${HADOOP_HOME}/etc/hadoop/${file}"
    done

    if [ -f "${CONFIG_DIR}/slaves" ]; then
      cp -f "${CONFIG_DIR}/slaves" "${HADOOP_HOME}/etc/hadoop/slaves"
      log "INFO" "已复制 slaves 文件"
    else
      log "WARN" "未找到 slaves 文件，跳过"
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      log "INFO" "启动NameNode"
      mkdir -p /data/hdfs/namenode

      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"

      nn_index=$(echo "${HOSTNAME}" | grep -o '[0-9]\+$')
      export HADOOP_NAMENODE_OPTS="$HADOOP_NAMENODE_OPTS -Ddfs.ha.namenode.id=nn${nn_index}"

      if [ ! -f /data/hdfs/namenode/formated ]; then
        if [ "$nn_index" = "0" ]; then
          log "INFO" "格式化第一个NameNode"
          "${HADOOP_HOME}/bin/hdfs" namenode -format -force -nonInteractive
          touch /data/hdfs/namenode/formated
        else
          log "INFO" "等待第一个NameNode格式化完成"
          timeout 10m bash -c "until [ -f /data/hdfs/namenode/formated ]; do sleep 10; done"
        fi
      fi

      "${HADOOP_HOME}/bin/hdfs" namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      log "INFO" "启动DataNode"
      mkdir -p /data/hdfs/datanode

      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"

      nn_ready=false
      REPLICA_COUNT={{ .Values.hdfs.nameNode.replicas | int }}
      for i in $(seq 0 $((REPLICA_COUNT - 1))); do
        nn_service="{{ include "hadoop.fullname" . }}-hdfs-nn-${i}}.{{ include "hadoop.fullname" . }}-hdfs-nn"
        tmp_url="http://${nn_service}:${NN_WEB_PORT}"
        for retry in {1..24}; do
          if curl -sf "${tmp_url}"; then
            log "INFO" "NameNode ${nn_service} 已就绪"
            nn_ready=true
            break 2
          fi
          sleep 5
        done
      done

      if [ "$nn_ready" = true ]; then
        "${HADOOP_HOME}/bin/hdfs" datanode
      else
        log "ERROR" "所有NameNode都未就绪，退出"
        exit 1
      fi
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-jn" ]]; then
      log "INFO" "启动JournalNode"
      mkdir -p /data/hdfs/journalnode
      "${HADOOP_HOME}/bin/hdfs" journalnode
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>{{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}{{ if $i }},{{ end }}nn{{ $i }}{{- end }}</value>
      </property>
      {{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}
      <property>
        <name>dfs.namenode.rpc-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9870</value>
      </property>
      {{- end }}
      <property>
        <name>dfs.client.failover.proxy.provider.{{ .Values.hdfs.clusterName }}</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>{{ .Values.hdfs.replication | default 1 }}</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>{{ .Values.hdfs.webhdfs.enabled | default false }}</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/hdfs/datanode</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/hdfs/namenode</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/hdfs/journalnode</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>{{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}{{ if $i }},{{ end }}nn{{ $i }}{{- end }}</value>
      </property>
      {{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}
      <property>
        <name>dfs.namenode.rpc-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn:9870</value>
      </property>
      {{- end }}
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{{- range $i, $e := until (.Values.hdfs.journalNode.replicas | int) }}{{ if $i }};{{ end }}{{ include "hadoop.fullname" $ }}-hdfs-jn-{{ $i }}.{{ include "hadoop.fullname" $ }}-journalnode:8485{{- end }}/{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
      </property>
      <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hadoop/.ssh/id_rsa</value>
      </property>
    </configuration>

  slaves: |
{{- range $i, $e := until (.Values.hdfs.dataNode.replicas | int) }}
    {{ include "hadoop.fullname" $ }}-hdfs-dn-{{ $i }}
{{- end }}
