apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}  # 配置映射名称，关联helm实例确保唯一性
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -eo pipefail  # 脚本出错立即退出，避免错误累积（增强容错）
    set -u  # 禁止使用未定义变量，减少逻辑错误

    # ==============================================
    # 环境初始化（统一管理路径与常量）
    # ==============================================
    : ${HADOOP_HOME:=/opt/hadoop}  # 默认Hadoop安装路径（可通过环境变量覆盖）
    : ${CONFIG_DIR:="/tmp/hadoop-config"}  # 配置文件挂载目录（ConfigMap挂载点）
    : ${NN_WEB_PORT:=9870}  # NameNode WebUI端口
    : ${DN_HTTP_PORT:=9864}  # DataNode HTTP端口（Hadoop 3.x默认）
    : ${DN_DATA_PORT:=9866}  # DataNode数据传输端口
    : ${JN_HTTP_PORT:=8480}  # JournalNode HTTP端口

    # 日志格式化函数（统一日志格式，便于排查）
    log() {
      local level=$1
      local message=$2
      echo "[$(date +'%Y-%m-%d %H:%M:%S')] [${level}] ${message}"
    }

    log "INFO" "启动Hadoop组件初始化，HADOOP_HOME=${HADOOP_HOME}"
    # 加载Hadoop环境变量（如JDK路径、内存配置等）
    if [ -f "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh" ]; then
      . "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh"
    else
      log "ERROR" "未找到hadoop-env.sh，路径：${HADOOP_HOME}/etc/hadoop/hadoop-env.sh"
      exit 1
    fi

    # ==============================================
    # 配置文件校验与复制（核心配置文件必须存在）
    # ==============================================
    log "INFO" "开始复制配置文件（来源：${CONFIG_DIR}）"
    local required_files=("slaves" "core-site.xml" "hdfs-site.xml")
    for file in "${required_files[@]}"; do
      if [ ! -f "${CONFIG_DIR}/${file}" ]; then
        log "ERROR" "缺少必需配置文件：${CONFIG_DIR}/${file}"
        exit 1
      fi
      # 复制并覆盖旧配置，检查复制结果
      if ! cp -f "${CONFIG_DIR}/${file}" "${HADOOP_HOME}/etc/hadoop/${file}"; then
        log "ERROR" "复制配置文件${file}失败"
        exit 1
      fi
      log "INFO" "已复制配置文件：${file}"
    done

    # ==============================================
    # 启动JournalNode（元数据同步组件）
    # ==============================================
    if [[ "${HOSTNAME}" =~ "hdfs-jn" ]]; then
      log "INFO" "检测到JournalNode角色，开始初始化"
      local jn_data_dir="/root/hdfs/journalnode"
      # 创建数据目录并设置权限（700：仅所有者可读写，增强安全性）
      mkdir -p "${jn_data_dir}" && chmod 700 "${jn_data_dir}"
      # 启动JournalNode，指定日志级别（从helm values获取）
      if ! "${HADOOP_HOME}/bin/hdfs" --loglevel {{ .Values.logLevel }} --daemon start journalnode; then
        log "ERROR" "JournalNode启动失败"
        exit 1
      fi
      log "INFO" "JournalNode启动成功，数据目录：${jn_data_dir}"
    fi

    # ==============================================
    # 启动NameNode（主节点，负责元数据管理）
    # ==============================================
    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      log "INFO" "检测到NameNode角色，开始初始化"
      local nn_data_dir="/root/hdfs/namenode"
      # 创建数据目录并设置权限
      mkdir -p "${nn_data_dir}" && chmod 700 "${nn_data_dir}"

      # 替换hdfs-site.xml中的端口占位符（仅NameNode需要处理，避免冗余操作）
      log "INFO" "替换端口占位符（DN_HTTP=${DN_HTTP_PORT}, DN_DATA=${DN_DATA_PORT}, JN_HTTP=${JN_HTTP_PORT}"
      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_JOURNALNODE_PORT_REPLACEME/${JN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"

      # 首次启动格式化NameNode（通过标记文件避免重复格式化）
      if [ ! -f "${nn_data_dir}/formated" ]; then
        log "INFO" "首次启动，开始格式化NameNode..."
        if ! "${HADOOP_HOME}/bin/hdfs" namenode -format -force -nonInteractive; then
          log "ERROR" "NameNode格式化失败"
          exit 1
        fi
        touch "${nn_data_dir}/formated"  # 创建标记文件
        log "INFO" "NameNode格式化完成"
      fi

      # 启动NameNode和ZKFC（ZKFC负责自动故障转移）
      if ! "${HADOOP_HOME}/bin/hdfs" --loglevel {{ .Values.logLevel }} --daemon start namenode; then
        log "ERROR" "NameNode启动失败"
        exit 1
      fi
      if ! "${HADOOP_HOME}/bin/hdfs" --loglevel {{ .Values.logLevel }} --daemon start zkfc; then
        log "ERROR" "ZKFC启动失败"
        exit 1
      fi
      log "INFO" "NameNode和ZKFC启动成功，数据目录：${nn_data_dir}"

      # ==============================================
      # 监控NN状态并动态更新Pod标签（active/standby）
      # ==============================================
      monitor_and_update_label() {
        # 配置常量（便于统一修改）
        local RETRY_MAX=3                # 最大重试次数
        local RETRY_DELAY=5              # 重试间隔（秒）
        local CHECK_INTERVAL=5           # 状态检查间隔（秒）
        local LABEL_KEY="hdfs.nn.state"  # 状态标签键名
        local ACTIVE_VALUE="active"      # 活跃状态标签值

        # 获取当前命名空间（从K8s内置secret读取）
        local NAMESPACE
        if ! NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace 2>/dev/null); then
          log "ERROR" "无法获取当前命名空间，退出监控"
          return 1
        fi

        # 获取当前Pod名称（与主机名一致）
        local POD_NAME="${HOSTNAME}"
        if [ -z "${POD_NAME}" ]; then
          log "ERROR" "无法获取Pod名称，退出监控"
          return 1
        fi

        # 解析NN ID（hdfs-nn-0 -> nn1，hdfs-nn-1 -> nn2，需与hdfs-site.xml匹配）
        local NN_ID
        NN_ID=$(echo "${POD_NAME}" | awk -F'-' '{print "nn" ($NF + 1)}')
        if [ -z "${NN_ID}" ] || [[ ! "${NN_ID}" =~ ^nn[1-2]$ ]]; then
          log "ERROR" "解析NN ID失败（Pod名称：${POD_NAME}），退出监控"
          return 1
        fi

        local LAST_STATE=""
        log "INFO" "启动NN状态监控（NN ID：${NN_ID}，Pod：${POD_NAME}）"

        # 循环检测状态
        while true; do
          # 获取当前NN状态（忽略临时错误输出）
          local CURRENT_STATE
          CURRENT_STATE=$("${HADOOP_HOME}/bin/hdfs" haadmin -getServiceState "${NN_ID}" 2>/dev/null)

          # 状态为空时跳过（可能是临时通信故障）
          if [ -z "${CURRENT_STATE}" ]; then
            log "WARN" "无法获取${NN_ID}状态，${CHECK_INTERVAL}秒后重试"
            sleep "${CHECK_INTERVAL}"
            continue
          fi

          # 仅在状态变化时执行更新
          if [ "${CURRENT_STATE}" != "${LAST_STATE}" ]; then
            log "INFO" "${NN_ID}状态变化：${LAST_STATE} -> ${CURRENT_STATE}"

            if [ "${CURRENT_STATE}" = "${ACTIVE_VALUE}" ]; then
              # 添加活跃标签（带重试机制）
              local success=0
              for ((i=1; i<=RETRY_MAX; i++)); do
                if kubectl patch pod "${POD_NAME}" -n "${NAMESPACE}" \
                  -p '{"metadata":{"labels":{"'${LABEL_KEY}'":"'${ACTIVE_VALUE}'"}}}' 2>/dev/null; then
                  log "INFO" "第${i}次尝试成功，为${POD_NAME}添加标签${LABEL_KEY}=${ACTIVE_VALUE}"
                  success=1
                  break
                else
                  log "WARN" "第${i}次尝试失败，无法为${POD_NAME}添加标签"
                  sleep "${RETRY_DELAY}"
                fi
              done
              if [ ${success} -eq 0 ]; then
                log "ERROR" "达到最大重试次数，${POD_NAME}标签更新失败"
              fi

            elif [ "${CURRENT_STATE}" = "standby" ]; then
              # 移除活跃标签（带重试机制）
              local success=0
              for ((i=1; i<=RETRY_MAX; i++)); do
                if kubectl patch pod "${POD_NAME}" -n "${NAMESPACE}" \
                  --type=json -p '[{"op":"remove","path":"/metadata/labels/'${LABEL_KEY}'"}]' 2>/dev/null; then
                  log "INFO" "第${i}次尝试成功，从${POD_NAME}移除标签${LABEL_KEY}"
                  success=1
                  break
                else
                  log "WARN" "第${i}次尝试失败，无法从${POD_NAME}移除标签"
                  sleep "${RETRY_DELAY}"
                fi
              done
              if [ ${success} -eq 0 ]; then
                log "ERROR" "达到最大重试次数，${POD_NAME}标签移除失败"
              fi

            else
              log "WARN" "未知状态${CURRENT_STATE}，不执行标签操作"
            fi

            # 更新状态记录（即使操作失败，也记录当前检测到的状态）
            LAST_STATE="${CURRENT_STATE}"
          fi

          sleep "${CHECK_INTERVAL}"
        done
      }

      # 后台运行状态监控函数
      monitor_and_update_label &
      log "INFO" "NN状态监控进程已启动"
    fi

    # ==============================================
    # 启动DataNode（数据存储节点）
    # ==============================================
    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      log "INFO" "检测到DataNode角色，开始初始化"
      local dn_data_dir="/root/hdfs/datanode"
      # 创建数据目录并设置权限
      mkdir -p "${dn_data_dir}" && chmod 700 "${dn_data_dir}"

      # 等待NameNode就绪（通过WebUI判断，超时5分钟）
      local nn_service="{{ include "hadoop.fullname" . }}-hdfs-nn"
      local tmp_url="http://${nn_service}:${NN_WEB_PORT}"
      log "INFO" "等待NameNode就绪（地址：${tmp_url}，超时5分钟）"

      if timeout 5m bash -c "
        while ! curl -sf '${tmp_url}'; do
          echo '[$(date +'%Y-%m-%d %H:%M:%S')] [INFO] 等待${tmp_url}...'
          sleep 5
        done
      "; then
        # 启动DataNode
        if ! "${HADOOP_HOME}/bin/hdfs" --loglevel {{ .Values.logLevel }} --daemon start datanode; then
          log "ERROR" "DataNode启动失败"
          exit 1
        fi
        log "INFO" "DataNode启动成功，数据目录：${dn_data_dir}"
      else
        log "ERROR" "等待NameNode超时（${tmp_url}），退出"
        exit 1
      fi
    fi

    # ==============================================
    # 容器存活保持（K8s容器需前台进程）
    # ==============================================
    if [[ $1 == "-d" ]]; then
      log "INFO" "进入后台运行模式，持续输出日志"
      # 等待日志文件生成（最多等待30秒）
      if ! timeout 30s bash -c "
        until find '${HADOOP_HOME}/logs' -mmin -1 | egrep -q '.*'; do
          echo '[$(date +'%Y-%m-%d %H:%M:%S')] [INFO] 等待日志文件生成...'
          sleep 2
        done
      "; then
        log "WARN" "超时未检测到日志文件，直接启动日志监控"
      fi
      # 持续监控日志（前台进程，防止容器退出）
      tail -F "${HADOOP_HOME}/logs/"* &
      wait $!  # 等待后台进程结束（实际不会结束，除非日志进程崩溃）
    elif [[ $1 == "-bash" ]]; then
      log "INFO" "进入交互模式"
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <!-- 默认文件系统URI，指向HDFS集群名称（与hdfs-site.xml的dfs.nameservices一致） -->
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ .Values.hdfs.clusterName }}</value>
            <description>NameNode URI，客户端通过此地址访问HDFS集群
            </description>
        </property>
        <!-- ZooKeeper集群地址（用于NN自动故障转移，需提前部署3+节点的ZK集群） -->
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>{{ .Values.hdfs.nameNode.zookeeperQuorum }}</value>
            <description>ZK节点列表，格式：host1:port,host2:port（默认端口2181）</description>
        </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <!-- HA配置：启用NameNode高可用 -->
        <property>
            <name>dfs.nameservices</name>
            <value>{{.Values.hdfs.clusterName}}</value>
            <description>HDFS集群逻辑名称（需与core-site.xml的fs.defaultFS一致）</description>
        </property>
        <property>
            <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
            <value>nn1,nn2</value>
            <description>两个NameNode的ID（需与实际Pod名称对应：nn1->hdfs-nn-0，nn2->hdfs-nn-1）</description>
        </property>

        <!-- 每个NN的HTTP访问地址（WebUI） -->
        <property>
            <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn1</name>
            <value>{{ include "hadoop.fullname" . }}
                -hdfs-nn-0:{{ .Values.hdfs.nameNode.webPort | default 9870 }}</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn2</name>
            <value>{{ include "hadoop.fullname" . }}
                -hdfs-nn-1:{{ .Values.hdfs.nameNode.webPort | default 9870 }}</value>
        </property>

        <!-- JournalNode地址：NN通过JN同步编辑日志（确保元数据一致，至少3节点） -->
        <property>
            <name>dfs.namenode.shared.edits.dir</name>
            <value>qjournal://{{ include "hadoop.fullname" . }}-hdfs-jn-0:8485;{{ include "hadoop.fullname" . }}
                -hdfs-jn-1:8485;{{ include "hadoop.fullname" . }}-hdfs-jn-2:8485/{{ .Values.hdfs.clusterName }}</value>
            <description>JN集群地址，格式：qjournal://jn1:port;jn2:port/nameservice</description>
        </property>

        <!-- 客户端故障转移代理：自动选择活跃NN -->
        <property>
            <name>dfs.client.failover.proxy.provider.{{ .Values.hdfs.clusterName }}</name>
            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
            <description>客户端用于发现活跃NN的代理类</description>
        </property>

        <!-- 启用自动故障转移（依赖ZKFC和ZK集群） -->
        <property>
            <name>dfs.ha.automatic-failover.enabled</name>
            <value>true</value>
            <description>开启基于ZK的自动故障转移（需部署ZKFC进程）</description>
        </property>

        {{- if .Values.hdfs.webhdfs.enabled -}}
        <!-- 启用WebHDFS（通过HTTP REST API访问HDFS，可选） -->
        <property>
            <name>dfs.webhdfs.enabled</name>
            <value>true</value>
        </property>
        {{- end -}}

        <!-- 节点通信配置（适配K8s动态IP环境） -->
        <property>
            <name>dfs.datanode.use.datanode.hostname</name>
            <value>false</value>
            <description>DN向NN注册时使用IP（而非主机名），适应K8s动态IP</description>
        </property>
        <property>
            <name>dfs.client.use.datanode.hostname</name>
            <value>false</value>
            <description>客户端访问DN时使用IP（而非主机名），减少DNS依赖</description>
        </property>

        <!-- 端口配置（占位符将被bootstrap.sh替换为实际值） -->
        <property>
            <name>dfs.datanode.http.address</name>
            <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
            <description>DN HTTP端口（Hadoop 3.x默认9864，2.x默认50075）</description>
        </property>
        <property>
            <name>dfs.datanode.address</name>
            <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
            <description>DN数据传输端口（Hadoop 3.x默认9866，2.x默认50010）</description>
        </property>
        <property>
            <name>dfs.journalnode.http-address</name>
            <value>0.0.0.0:EXTERNAL_JOURNALNODE_PORT_REPLACEME</value>
            <description>JN HTTP端口（默认8480，数据同步端口为8485）</description>
        </property>

        <!-- 数据副本数（需与DN副本数匹配，默认3确保可靠性） -->
        <property>
            <name>dfs.replication</name>
            <value>3</value>
            <description>默认数据块副本数，建议与DataNode副本数一致</description>
        </property>

        <!-- 客户端故障转移重试配置 -->
        <property>
            <name>dfs.client.failover.connection.retries</name>
            <value>3</value>
            <description>故障转移时的连接重试次数</description>
        </property>
        <property>
            <name>dfs.client.failover.connection.timeout.ms</name>
            <value>5000</value>
            <description>故障转移时的连接超时时间（毫秒）</description>
        </property>

        <!-- 数据存储目录（与容器内挂载路径一致，需绑定PVC确保持久化） -->
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:///root/hdfs/datanode</value>
            <description>DataNode数据存储目录（需与volumeMounts路径匹配）</description>
        </property>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:///root/hdfs/namenode</value>
            <description>NameNode元数据目录（需持久化，否则集群重启后数据丢失）</description>
        </property>

        <!-- 关闭主机名检查（适应K8s动态IP与主机名映射） -->
        <property>
            <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
            <value>false</value>
            <description>关闭NN对DN的IP-主机名一致性检查，避免K8s中DNS延迟导致注册失败</description>
        </property>

        <!-- 绑定所有网络接口（允许容器内部访问） -->
        <property>
            <name>dfs.namenode.rpc-bind-host</name>
            <value>0.0.0.0</value>
            <description>NN的RPC服务绑定所有网络接口（K8s中需跨Pod访问）</description>
        </property>
        <property>
            <name>dfs.namenode.servicerpc-bind-host</name>
            <value>0.0.0.0</value>
            <description>NN的Service RPC服务（主从同步）绑定所有网络接口</description>
        </property>
    </configuration>

  slaves: |
    # 传统Hadoop中用于定义DataNode节点列表，K8s环境下由StatefulSet动态管理DataNode，此文件仅为兼容保留
    localhost