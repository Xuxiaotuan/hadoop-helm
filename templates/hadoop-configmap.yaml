apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -eo pipefail
    set -u

    HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop}
    CONFIG_DIR="/tmp/hadoop-config"
    NN_WEB_PORT=9870
    DN_HTTP_PORT=9864
    DN_DATA_PORT=9866

    log() {
      level=$1
      message=$2
      echo "[$(date +'%Y-%m-%d %H:%M:%S')] [${level}] ${message}"
    }

    log "INFO" "启动Hadoop组件初始化，HADOOP_HOME=${HADOOP_HOME}"

    if [ -f "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh" ]; then
      . "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh"
    else
      log "ERROR" "未找到hadoop-env.sh"
      exit 1
    fi

    log "INFO" "复制配置文件"
    required_files=("core-site.xml" "hdfs-site.xml")
    for file in "${required_files[@]}"; do
      if [ ! -f "${CONFIG_DIR}/${file}" ]; then
        log "ERROR" "缺少配置文件 ${file}"
        exit 1
      fi
      cp -f "${CONFIG_DIR}/${file}" "${HADOOP_HOME}/etc/hadoop/${file}"
    done

    if [ -f "${CONFIG_DIR}/slaves" ]; then
      cp -f "${CONFIG_DIR}/slaves" "${HADOOP_HOME}/etc/hadoop/slaves"
      log "INFO" "已复制 slaves 文件"
    else
      log "WARN" "未找到 slaves 文件，跳过"
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      log "INFO" "启动NameNode"
      mkdir -p /data/hdfs/namenode

      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"

      # 获取namenode索引
      nn_index=$(echo "${HOSTNAME}" | grep -o '[0-9]\+$')
      
      if [ ! -f /data/hdfs/namenode/formated ]; then
        if [ "$nn_index" = "0" ]; then
          log "INFO" "格式化第一个NameNode"
          "${HADOOP_HOME}/bin/hdfs" namenode -format -force -nonInteractive
          touch /data/hdfs/namenode/formated
        else
          log "INFO" "等待第一个NameNode格式化完成"
          # 等待第一个namenode格式化完成
          timeout 10m bash -c "until [ -f /data/hdfs/namenode/formated ]; do sleep 10; done"
        fi
      fi

      # 前台启动 NameNode
      "${HADOOP_HOME}/bin/hdfs" namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      log "INFO" "启动DataNode"
      mkdir -p /data/hdfs/datanode
      
      # 等待任意一个namenode就绪
      nn_services=("{{ include "hadoop.fullname" . }}-hdfs-nn-0" "{{ include "hadoop.fullname" . }}-hdfs-nn-1" "{{ include "hadoop.fullname" . }}-hdfs-nn-2")
      nn_ready=false
      
      for nn_service in "${nn_services[@]}"; do
        tmp_url="http://${nn_service}:${NN_WEB_PORT}"
        if timeout 2m bash -c "until curl -sf '${tmp_url}'; do sleep 5; done"; then
          log "INFO" "NameNode ${nn_service} 已就绪"
          nn_ready=true
          break
        fi
      done
      
      if [ "$nn_ready" = true ]; then
        # 前台启动 DataNode，日志直接输出到 stdout
        "${HADOOP_HOME}/bin/hdfs" datanode
      else
        log "ERROR" "所有NameNode都未就绪，退出"
        exit 1
      fi
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-jn" ]]; then
      log "INFO" "启动JournalNode"
      mkdir -p /data/hdfs/journalnode
      
      # 前台启动 JournalNode
      "${HADOOP_HOME}/bin/hdfs" journalnode
    fi

    if [[ $1 == "-d" ]]; then
      log "INFO" "保持容器运行"
      tail -F "${HADOOP_HOME}/logs/"* &
      wait $!
    elif [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{{ .Values.hdfs.rpcAddress }}</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>nn0,nn1,nn2</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.{{ .Values.hdfs.clusterName }}.nn0</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-0:9000</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.{{ .Values.hdfs.clusterName }}.nn1</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-1:9000</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.{{ .Values.hdfs.clusterName }}.nn2</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-2:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn0</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-0:9870</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn1</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-1:9870</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn2</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-2:9870</value>
      </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>{{ .Values.hdfs.replication | default 3 }}</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>{{ .Values.hdfs.webhdfs.enabled | default false }}</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/hdfs/datanode</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/hdfs/namenode</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address</name>
        <value>0.0.0.0:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address</name>
        <value>0.0.0.0:9870</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>nn0,nn1,nn2</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.{{ .Values.hdfs.clusterName }}.nn0</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-0:9000</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.{{ .Values.hdfs.clusterName }}.nn1</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-1:9000</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.{{ .Values.hdfs.clusterName }}.nn2</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-2:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn0</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-0:9870</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn1</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-1:9870</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ .Values.hdfs.clusterName }}.nn2</name>
        <value>{{ include "hadoop.fullname" . }}-hdfs-nn-2:9870</value>
      </property>
    </configuration>

  slaves: |
    localhost

