apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -eo pipefail
    set -u

    HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop}
    CONFIG_DIR="/tmp/hadoop-config"
    NN_WEB_PORT=9870
    DN_HTTP_PORT=9864
    DN_DATA_PORT=9866
    JN_HTTP_PORT=8480
    JN_RPC_PORT=8485
    KUBECTL_TIMEOUT=10s

    log() {
      level=$1
      message=$2
      echo "[$(date +'%Y-%m-%d %H:%M:%S')] [${level}] ${message}"
    }

    # 目录初始化（带权限检查）
    init_dir() {
      local dir=$1
      mkdir -p "${dir}" || { log "ERROR" "创建目录失败: ${dir}"; exit 1; }
      chmod 700 "${dir}" || { log "ERROR" "设置目录权限失败: ${dir}"; exit 1; }
      touch "${dir}/.test" && rm -f "${dir}/.test" || { log "ERROR" "目录不可写: ${dir}"; exit 1; }
    }

    # 等待端口就绪
    wait_for_port() {
      local host=$1
      local port=$2
      local timeout=$3
      log "INFO" "等待 $host:$port 就绪（超时 $timeout）"
      if ! timeout "$timeout" bash -c "while ! nc -z $host $port; do sleep 2; done"; then
        log "ERROR" "$host:$port 超时未就绪"
        exit 1
      fi
    }

    log "INFO" "启动Hadoop组件初始化，HADOOP_HOME=${HADOOP_HOME}"

    mkdir -p /opt/hadoop/logs
    chmod 777 /opt/hadoop/logs

    if [ -f "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh" ]; then
      . "${HADOOP_HOME}/etc/hadoop/hadoop-env.sh"
    else
      log "ERROR" "未找到hadoop-env.sh"
      exit 1
    fi

    log "INFO" "复制配置文件"
    required_files=("core-site.xml" "hdfs-site.xml")
    for file in "${required_files[@]}"; do
      if [ ! -f "${CONFIG_DIR}/${file}" ]; then
        log "ERROR" "缺少配置文件 ${file}"
        exit 1
      fi
      cp -f "${CONFIG_DIR}/${file}" "${HADOOP_HOME}/etc/hadoop/${file}"
    done

    if [ -f "${CONFIG_DIR}/slaves" ]; then
      cp -f "${CONFIG_DIR}/slaves" "${HADOOP_HOME}/etc/hadoop/slaves"
      log "INFO" "已复制 slaves 文件"
    else
      log "WARN" "未找到 slaves 文件，跳过"
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      log "INFO" "启动DataNode"
      log "INFO" "DataNode配置：HTTP端口=${DN_HTTP_PORT}, 数据端口=${DN_DATA_PORT}"
      init_dir "/data/hdfs/datanode"

      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      log "INFO" "更新配置文件端口配置"

      # 设置DataNode主机名
      dn_index=$(echo "${HOSTNAME}" | grep -o '[0-9]\+$')
      export HDFS_DATANODE_OPTS="${HDFS_DATANODE_OPTS:-} -Ddfs.datanode.hostname=${HOSTNAME}"
      log "INFO" "设置DataNode主机名为: ${HOSTNAME}"

      # 等待NameNode就绪
      nn_service="{{ include "hadoop.fullname" . }}-hdfs-nn-active"
      wait_for_port "$nn_service" "$NN_WEB_PORT" "5m"

      # 检查是否需要强制格式化DataNode
      FORCE_FORMAT_DN={{ .Values.formatting.forceFormatDataNode | default false }}
      if [ "$FORCE_FORMAT_DN" = "true" ] || [ ! -f /data/hdfs/datanode/current/VERSION ]; then
        log "INFO" "清理DataNode存储目录"
        pkill -f datanode || true
        sleep 2
        rm -rf /data/hdfs/datanode/current
        rm -rf /data/hdfs/datanode/in_use.lock
        log "INFO" "DataNode存储目录已清理，将重新注册到NameNode"
      else
        log "INFO" "DataNode已格式化，跳过清理步骤"
      fi

      log "INFO" "启动DataNode服务"
      log "INFO" "DataNode将连接到NameNode进行注册"
      "${HADOOP_HOME}/bin/hdfs" datanode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-jn" ]]; then
      log "INFO" "启动JournalNode"
      log "INFO" "JournalNode用于存储NameNode的编辑日志，支持HA"
      init_dir "/data/hdfs/journalnode"
      FORCE_FORMAT_JN={{ .Values.formatting.forceFormatJournalNode | default false }}
      if [ "$FORCE_FORMAT_JN" = "true" ] || [ ! -f /data/hdfs/journalnode/min-hadoop/current/VERSION ]; then
        log "INFO" "格式化JournalNode存储目录"
        pkill -f journalnode || true
        sleep 2
        rm -rf /data/hdfs/journalnode/min-hadoop
        mkdir -p /data/hdfs/journalnode/min-hadoop
        "${HADOOP_HOME}/bin/hdfs" journalnode -format
        log "INFO" "JournalNode格式化完成"
      else
        log "INFO" "JournalNode已格式化，跳过格式化步骤"
      fi
      log "INFO" "启动JournalNode服务"
      log "INFO" "JournalNode将在端口8480提供HTTP服务，8485提供RPC服务"
      "${HADOOP_HOME}/bin/hdfs" journalnode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      log "INFO" "启动NameNode"
      log "INFO" "NameNode是HDFS的主节点，负责元数据管理"
      init_dir "/data/hdfs/namenode"
      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${DN_HTTP_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${DN_DATA_PORT}/" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      log "INFO" "更新DataNode端口配置"
      nn_index=$(echo "${HOSTNAME}" | grep -o '[0-9]\+$')
      export HDFS_NAMENODE_OPTS="${HDFS_NAMENODE_OPTS:-} -Ddfs.ha.namenode.id=nn${nn_index} -Ddfs.namenode.rpc-bind-host=0.0.0.0 -Ddfs.namenode.http-bind-host=0.0.0.0"
      log "INFO" "设置NameNode ID: nn${nn_index}"
      log "INFO" "设置NameNode绑定地址: 0.0.0.0"
      sed -i "s/<value>nn\${nn_index}<\/value>/<value>nn${nn_index}<\/value>/g" "${HADOOP_HOME}/etc/hadoop/hdfs-site.xml"
      log "INFO" "已更新hdfs-site.xml中的namenode.id为: nn${nn_index}"

      # 等待所有JournalNode就绪
      log "INFO" "等待所有JournalNode就绪..."
      jn_ready=false
      JN_REPLICA_COUNT={{ .Values.hdfs.journalNode.replicas | int }}
      log "INFO" "需要等待所有 ${JN_REPLICA_COUNT} 个JournalNode就绪"
      for i in $(seq 0 $((JN_REPLICA_COUNT - 1))); do
        jn_service="{{ include "hadoop.fullname" . }}-hdfs-jn-${i}.{{ include "hadoop.fullname" . }}-hdfs-jn"
        wait_for_port "$jn_service" "$JN_RPC_PORT" "5m"
      done

      # NameNode格式化加锁机制
      lock_file="/data/hdfs/namenode/.format.lock"
      if [ ! -f "/data/hdfs/namenode/formated" ]; then
        if mkdir "${lock_file}" 2>/dev/null; then
          if [ "$nn_index" = "0" ]; then
            log "INFO" "格式化第一个NameNode (Active)"
            log "INFO" "这将创建新的HDFS文件系统"
            rm -rf /data/hdfs/namenode/current
            rm -rf /data/hdfs/namenode/formated
            "${HADOOP_HOME}/bin/hdfs" namenode -format -force -nonInteractive
            if [ $? -ne 0 ]; then
              log "ERROR" "NameNode格式化失败"
              rm -rf "${lock_file}"
              exit 1
            fi
            touch /data/hdfs/namenode/formated
            log "INFO" "NameNode格式化完成"
          else
            log "INFO" "启动第二个NameNode（Standby模式）"
            rm -rf /data/hdfs/namenode/current
            rm -rf /data/hdfs/namenode/formated
            log "INFO" "执行bootstrapStandby命令同步元数据..."
            "${HADOOP_HOME}/bin/hdfs" namenode -bootstrapStandby
            if [ $? -ne 0 ]; then
              log "ERROR" "bootstrapStandby失败，无法同步元数据"
              rm -rf "${lock_file}"
              exit 1
            fi
            touch /data/hdfs/namenode/formated
            log "INFO" "第二个NameNode元数据同步完成"
          fi
          rm -rf "${lock_file}"
        else
          log "INFO" "等待其他节点完成格式化..."
          while [ ! -f "/data/hdfs/namenode/formated" ]; do
            sleep 5
          done
        fi
      fi

      # 启动NameNode服务
      log "INFO" "启动NameNode服务"
      log "INFO" "NameNode将在端口9000提供RPC服务，9870提供HTTP服务"
      "${HADOOP_HOME}/bin/hdfs" namenode &
      namenode_pid=$!
      log "INFO" "等待NameNode启动..."
      for retry in {1..60}; do
        if curl -sf "http://localhost:9870" >/dev/null 2>&1; then
          log "INFO" "NameNode已启动，等待30秒确保完全就绪..."
          sleep 30
          break
        else
          log "INFO" "NameNode未就绪，继续等待 (重试 ${retry}/60)"
          sleep 5
        fi
      done

      # 自动标签切换（仅在容器内有kubectl时启用）
      if command -v kubectl &>/dev/null; then
        monitor_nn_state() {
          local LABEL_KEY="hdfs.nn.state"
          local ACTIVE="active"
          local NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace 2>/dev/null || echo "default")
          local POD_NAME="${HOSTNAME}"
          local NN_ID="nn${nn_index}"
          log "INFO" "启动自动标签监控 (NN_ID=${NN_ID})"
          while sleep 5; do
            local STATE=$("${HADOOP_HOME}/bin/hdfs" haadmin -getServiceState "${NN_ID}" 2>/dev/null)
            if [ -z "${STATE}" ]; then
              continue
            fi
            if [ "${STATE}" = "${ACTIVE}" ]; then
              if ! kubectl get pod "${POD_NAME}" -n "${NAMESPACE}" -l "${LABEL_KEY}=${ACTIVE}" --no-headers &>/dev/null; then
                kubectl patch pod "${POD_NAME}" -n "${NAMESPACE}" -p '{"metadata":{"labels":{"'${LABEL_KEY}'":"'${ACTIVE}'"}}}' --timeout="${KUBECTL_TIMEOUT}" &>/dev/null
                log "INFO" "已自动打标签: ${LABEL_KEY}=${ACTIVE}"
              fi
            else
              # standby时移除active标签
              if kubectl get pod "${POD_NAME}" -n "${NAMESPACE}" -l "${LABEL_KEY}=${ACTIVE}" --no-headers &>/dev/null; then
                kubectl patch pod "${POD_NAME}" -n "${NAMESPACE}" --type=json -p '[{"op":"remove","path":"/metadata/labels/'${LABEL_KEY}'"}]' --timeout="${KUBECTL_TIMEOUT}" &>/dev/null
                log "INFO" "已自动移除标签: ${LABEL_KEY}"
              fi
            fi
          done
        }
        monitor_nn_state &
      else
        log "WARN" "未找到kubectl，跳过自动标签切换"
      fi

      if [ "$nn_index" = "0" ]; then
        log "INFO" "设置nn0为Active NameNode"
        sleep 10
        for retry in {1..30}; do
          if "${HADOOP_HOME}/bin/hdfs" haadmin -transitionToActive nn0 -forceactive 2>/dev/null; then
            log "INFO" "成功设置nn0为Active NameNode"
            break
          else
            log "INFO" "设置Active失败，重试 (${retry}/30)"
            sleep 5
          fi
        done
        if "${HADOOP_HOME}/bin/hdfs" haadmin -getServiceState nn0 2>/dev/null | grep -q "active"; then
          log "INFO" "nn0 Active状态确认成功"
        else
          log "WARN" "nn0 Active状态确认失败，但继续运行"
        fi
      fi
      wait $namenode_pid
    fi

    if [[ "${1:-}" == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.{{ .Values.hdfs.clusterName }}</name>
        <value>{{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}{{ if $i }},{{ end }}nn{{ $i }}{{- end }}</value>
      </property>
      {{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}
      <property>
        <name>dfs.namenode.rpc-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn.{{ $.Release.Namespace }}.svc.cluster.local:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn.{{ $.Release.Namespace }}.svc.cluster.local:9870</value>
      </property>
      {{- end }}
      <property>
        <name>dfs.client.failover.proxy.provider.{{ .Values.hdfs.clusterName }}</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>{{ .Values.hdfs.replication | default 3 }}</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>{{ .Values.hdfs.webhdfs.enabled | default false }}</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/hdfs/datanode</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/hdfs/namenode</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/hdfs/journalnode</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
      </property>
      <property>
        <name>dfs.datanode.hostname</name>
        <value>${HOSTNAME}</value>
      </property>
      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>
      {{- range $i, $e := until (.Values.hdfs.nameNode.replicas | int) }}
      <property>
        <name>dfs.namenode.rpc-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>{{ include "hadoop.fullname" $ }}-hdfs-nn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-nn.{{ $.Release.Namespace }}.svc.cluster.local:9000</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.{{ $.Values.hdfs.clusterName }}.nn{{ $i }}</name>
        <value>0.0.0.0:9870</value>
      </property>
      {{- end }}
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{{- range $i, $e := until (.Values.hdfs.journalNode.replicas | int) }}{{ if $i }};{{ end }}{{ include "hadoop.fullname" $ }}-hdfs-jn-{{ $i }}.{{ include "hadoop.fullname" $ }}-hdfs-jn.{{ $.Release.Namespace }}.svc.cluster.local:8485{{- end }}/{{ .Values.hdfs.clusterName }}</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
      </property>
      <property>
        <name>dfs.ha.initialization.timeout</name>
        <value>300</value>
      </property>
      <property>
        <name>dfs.ha.standby.checkpoints</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.ha.namenode.id</name>
        <value>nn${nn_index}</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.https-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>

    </configuration>

  slaves: |
{{- range $i, $e := until (.Values.hdfs.dataNode.replicas | int) }}
    {{ include "hadoop.fullname" $ }}-hdfs-dn-{{ $i }}
{{- end }} 